{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2c619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032a1657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 15:43:47.088563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11124 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:3b:00.0, compute capability: 8.6\n",
      "2024-03-12 15:43:47.089398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 90 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:af:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "blooms_model = tf.keras.models.load_model('/LAB/blooms/blooms_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1499c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.38.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64340bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122703eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddfed625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from transformers import BertTokenizer\n",
    "# import tensorflow as tf\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# def prepare_data(input_text, tokenizer):\n",
    "#     token = tokenizer.encode_plus(\n",
    "#         input_text,\n",
    "#         max_length=256,\n",
    "#         truncation=True,\n",
    "#         padding='max_length',\n",
    "#         add_special_tokens=True,\n",
    "#         return_tensors='tf'\n",
    "#     )\n",
    "#     return {\n",
    "#         'input_ids': tf.cast(token.input_ids, tf.float64),\n",
    "#         'attention_mask': tf.cast(token.attention_mask, tf.float64)\n",
    "#     }\n",
    "\n",
    "# def make_prediction(model, processed_data, classes=['remember', 'understand', 'apply', 'analyze', 'evaluate','create']):\n",
    "#     probs = model.predict(processed_data)[0]\n",
    "#     return classes[np.argmax(probs)], np.argmax(probs) + 1  \n",
    "\n",
    "# def read_questions_from_csv(filename):\n",
    "#     df = pd.read_csv(filename)\n",
    "#     questions = df['Questions'].tolist()  \n",
    "#     return df, questions\n",
    "\n",
    "# csv_filename = '/LAB/blooms/question_daa.csv'\n",
    "# df, questions = read_questions_from_csv(csv_filename)\n",
    "\n",
    "# predicted_blooms_levels = []\n",
    "# predicted_blooms_encoded = []  \n",
    "# for i, question in enumerate(questions):\n",
    "#     processed_data = prepare_data(question, tokenizer)\n",
    "#     result, encoded_value = make_prediction(blooms_model, processed_data=processed_data)\n",
    "#     predicted_blooms_levels.append(result)\n",
    "#     predicted_blooms_encoded.append(encoded_value)\n",
    "\n",
    "# df['Predicted Blooms Level'] = predicted_blooms_levels\n",
    "# df['Encoded Blooms Level'] = predicted_blooms_encoded  \n",
    "\n",
    "# output_csv_filename = 'output.csv' \n",
    "# df[['Questions', 'Predicted Blooms Level', 'Encoded Blooms Level']].to_csv(output_csv_filename, index=False)\n",
    "\n",
    "# print(f\"Predicted blooms levels along with questions have been stored in {output_csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04e516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def prepare_data(input_text, tokenizer):\n",
    "    token = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tf.cast(token.input_ids, tf.float64),\n",
    "        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n",
    "    }\n",
    "\n",
    "def make_prediction(model, processed_data, classes=['remember', 'understand', 'apply', 'analyze', 'evaluate','create']):\n",
    "    probs = model.predict(processed_data)[0]\n",
    "    return classes[np.argmax(probs)], np.argmax(probs) + 1  \n",
    "\n",
    "def read_questions_from_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    questions = df['Questions'].tolist()  \n",
    "    return df, questions\n",
    "\n",
    "def calculate_question_paper_quality(df):\n",
    "    full_marks = df['Marks'].sum()  \n",
    "    df['Encoded_Blooms_Level'] = df['Encoded_Blooms_Level'].astype(float)\n",
    "    df['Question_Weightage'] = df['Marks'].astype(float)\n",
    "    df['Question_Paper_Quality'] = (df['Encoded_Blooms_Level'] * df['Question_Weightage']) / full_marks\n",
    "    return df['Question_Paper_Quality'].sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51e6232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted blooms levels along with questions have been stored in output_final.csv\n",
      "Score of the question paper is: 3.4444444444444438\n",
      "The difficulty  of the question paper is: Medium\n"
     ]
    }
   ],
   "source": [
    "def categorize_difficulty(difficulty_score):\n",
    "    if difficulty_score <= 1.5:\n",
    "        return \"Easy\"\n",
    "    elif 1.5 < difficulty_score <= 3.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "csv_filename = '/LAB/blooms/final_output-daa.csv'\n",
    "df, questions = read_questions_from_csv(csv_filename)\n",
    "\n",
    "predicted_blooms_levels = []\n",
    "predicted_blooms_encoded = [] \n",
    "for i, question in enumerate(questions):\n",
    "    processed_data = prepare_data(question, tokenizer)\n",
    "    result, encoded_value = make_prediction(blooms_model, processed_data=processed_data)\n",
    "    predicted_blooms_levels.append(result)\n",
    "    predicted_blooms_encoded.append(encoded_value)\n",
    "\n",
    "df['Predicted_Blooms_Level'] = predicted_blooms_levels\n",
    "df['Encoded_Blooms_Level'] = predicted_blooms_encoded  \n",
    "\n",
    "output_csv_filename = 'output_final.csv'  \n",
    "df[['Questions', 'Predicted_Blooms_Level', 'Encoded_Blooms_Level']].to_csv(output_csv_filename, index=False)\n",
    "\n",
    "question_paper_quality = calculate_question_paper_quality(df)\n",
    "\n",
    "difficulty_code = categorize_difficulty(question_paper_quality)\n",
    "\n",
    "# df['Difficulty_Code'] = difficulty_code\n",
    "\n",
    "df.to_csv(output_csv_filename, index=False)\n",
    "\n",
    "print(f\"Predicted blooms levels along with questions have been stored in {output_csv_filename}\")\n",
    "print(f\"Score of the question paper is: {question_paper_quality}\")\n",
    "print(f\"The difficulty  of the question paper is: {difficulty_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aede99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
